---
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

## Work breakdown

-   *Nazar Pasichnyk*:Classifier implementation, part of conclusion
-   *Maxim Holovin*:Data pre-processing, part of conclusion
-   *Sofia Pereima*:Data visualization, Measure effectiveness of your classifier

## Introduction

During the first three weeks, you learned a couple of essential notions
and theorems, and one of the most important among them is the **Bayes
theorem**.

**Naive Bayes Classifier** is a simple algorithm, which is based on
**Bayes theorem** and used for solving classification problems.
**Classification problem** is a problem in which an observation has to
be classified in one of the $n$ classes based on its similarity with
observations in each class.

It is a **probabilistic classifier**, which means it predicts based on
the probability of an observation belonging to each class. To compute
it, this algorithm uses **Bayes' formula,** that you probably already
came across in **Lesson 3:**
$$\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}$$

Under the strong **independence** assumption, one can calculate
$\mathsf{P}(\mathrm{observation} \mid \mathrm{class})$ as
$$\mathsf{P}(\mathrm{observation}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i), \qquad \mathsf{P}(\mathrm{observation} \mid \mathrm{class}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i \mid \mathrm{class}),$$
where $n$ is the total number of features describing a given observation
(*For example, if an observation is presented as a sentence, then each
word can be a feature*). Thus,
$\mathsf{P}(\mathrm{class}|\mathrm{observation})$ now can be calculated
as

$$\mathsf{P}(\mathrm{class} \mid \mathrm{\mathrm{observation}}) = \mathsf{P}(\mathrm{class})\times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i\mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}\tag{1}$$

All the terms on the right-hand side can be estimated as corresponding
relative frequencies using available data\

**See [*this
link*](https://www.javatpoint.com/machine-learning-naive-bayes-classifier)
for more detailed explanations & examples :) Also you can watch [*this
video*](https://youtu.be/O2L2Uv9pdDA?si=-ohkHVDuu3sLLGMq) for more
examples!**

## Data description

There are 5 datasets uploaded on the cms (data.zip)

To determine your variant, take your team number from the list of teams
on cms and take *mod 5* - this is the number of your data set.

-   **0 - authors** This data set consists of citations of three famous
    writers: Edgar Alan Poe, Mary Wollstonecraft Shelley and HP
    Lovecraft. The task with this data set is to classify a piece of
    text with the author who was more likely to write it.

-   **1 - discrimination** This data set consists of tweets that have
    discriminatory (sexism or racism) messages or of tweets that are of
    neutral mood. The task is to determine whether a given tweet has
    discriminatory mood or does not.

-   **2 - fake news** This data set contains data of American news: a
    headline and an abstract of the article. Each piece of news is
    classified as fake or credible. The task is to classify the news
    from test.csv as credible or fake.

-   **3 - sentiment** All the text messages contained in this data set
    are labeled with three sentiments: positive, neutral or negative.
    The task is to classify some text message as the one of positive
    mood, negative or neutral.

-   **4 - spam** This last data set contains SMS messages classified as
    spam or non-spam (ham in the data set). The task is to determine
    whether a given message is spam or non-spam.

Each data set consists of two files: *train.csv* and *test.csv*. The
first one is used to find the probabilities of the corresponding classes
and the second one is used to test your classifier afterwards. Note that
it is crucial to randomly split your data into training and testing
parts to test the classifierʼs possibilities on the unseen data.

```{r}
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
library(tm)
library(stringr)
library(tidyverse)
library(wordcloud)
library(reshape2)
```

## Outline of the work

1.  **Data pre-processing** (includes removing punctuation marks and
    stop words, representing each message as a bag-of-words)
2.  **Data visualization** (it's time to plot your data!)
3.  **Classifier implementation** (using the training set, calculate all
    the conditional probabilities in formula (1))
4.  **Measurements of effectiveness of your classifier** (use the
    results from the previous step to predict classes for messages in
    the testing set and measure the accuracy, precision and recall, F1
    score metric etc)
5.  **Conclusions**

*!! do not forget to submit both the (compiled) Rmd source file and the
.html output !!*

## Data pre-processing

-   Read the *.csv* data files.
-   Сlear your data from punctuation or other unneeded symbols.
-   Clear you data from stop words. You don't want words as is, and, or
    etc. to affect your probabilities distributions, so it is a wise
    decision to get rid of them. Find list of stop words in the cms
    under the lab task.
-   Represent each test message as its bag-of-words. Here:
    <https://machinelearningmastery.com/gentle-introduction-bag-words-model/>
    you can find general introduction to the bag-of-words model and
    examples on to create it.
-   It is highly recommended to get familiar with R dataframes, it would
    make the work much easier to do.
-   Useful links:
    -   <https://steviep42.github.io/webscraping/book/bagofwords.html#tidytext> -
        example of using *tidytext* to count frequencies of the words.
    -   Basics of Text Mining in R:
        <http://rstudio-pubs-static.s3.amazonaws.com/256588_57b585da6c054349825cba46685d8464.html>
        . Note that it also includes an example on how to create a bag
        of words from your text document.

```{r}
list.files(getwd())
list.files("2-fake_news")
```

```{r}
test_path <- "2-fake_news/test.csv"
train_path <- "2-fake_news/train.csv"

stop_words <- read_file("stop_words.txt")
# https://stackoverflow.com/questions/27195912/why-does-strsplit-return-a-list
splitted_stop_words <- strsplit(stop_words, split='\n')
splitted_stop_words <- splitted_stop_words[[1]]
```

```{r}
train <-  read.csv(file = train_path, stringsAsFactors = FALSE)
test <-  read.csv(file = test_path, stringsAsFactors = FALSE)
```

```{r}
clean_text <- function(text) {
  text %>%
    tolower() %>%
    str_replace_all("[[:punct:]]", " ") %>%
    str_replace_all("[[:digit:]]", " ") %>%
    str_replace_all("\\s+", " ") %>%
    str_trim()
}
train[["cleaned_text"]] <- sapply(train[["Body"]], clean_text)
test[["cleaned_text"]]  <- sapply(test[["Body"]], clean_text)


tidy_train <- train %>%
  mutate(doc_id = row_number()) %>%
  unnest_tokens(word, cleaned_text, token = "words") %>%
  filter(!word %in% splitted_stop_words)

tidy_test <- test %>%
  mutate(doc_id = row_number()) %>%
  unnest_tokens(word, cleaned_text, token = "words") %>%
  filter(!word %in% splitted_stop_words)
```
```{r}

# Create word counts and document-term matrix for training data
train_word_counts <- tidy_train %>% count(doc_id, word)

train_dtm <- train_word_counts %>% cast_dtm(doc_id, word, n)
X_train_raw <- as.matrix(train_dtm)

# Create full training matrix that includes all documents
X_train <- matrix(0, nrow = nrow(train), ncol = ncol(X_train_raw))
colnames(X_train) <- colnames(X_train_raw)
rownames(X_train) <- 1:nrow(train)

# Fill in the word counts for documents that have words
X_train[as.integer(rownames(X_train_raw)), ] <- X_train_raw

train_vocabulary <- colnames(X_train)
cat("Vocabulary size:", length(train_vocabulary), "\n")

# Create word counts and document-term matrix for test data
test_word_counts <- tidy_test %>% 
  count(doc_id, word) %>%
  filter(word %in% train_vocabulary)

test_dtm <- test_word_counts %>% cast_dtm(doc_id, word, n)
X_test_raw <- as.matrix(test_dtm)

# Create full test matrix that includes all documents
X_test <- matrix(0, nrow = nrow(test), ncol = ncol(X_train))
colnames(X_test) <- colnames(X_train)
rownames(X_test) <- 1:nrow(test)

# Fill in the word counts for documents that have words
if (nrow(X_test_raw) > 0) {
  matching_cols <- intersect(colnames(X_test_raw), colnames(X_train))
  X_test[as.integer(rownames(X_test_raw)), matching_cols] <- X_test_raw[, matching_cols]
}

y_train <- ifelse(train[["Label"]] == "credible", 0, 1)
y_test  <- ifelse(test[["Label"]] == "credible", 0, 1)
```

## Data visualization

Each time you work with some data, you need to understand it before you
start processing it. R has very powerful tools to make nice plots and
visualization. Show what are the most common words for negative and
positive examples as a histogram, word cloud etc. Be creative!

```{r}
stop_words <- splitted_stop_words
visualize_wordcloud <- function(data, class_name, stop_words) {
  text_vector <- data$Body[data$Label == class_name]
  
  words <- unlist(strsplit(tolower(text_vector), "\\W"))

  words <- words[!(words %in% stop_words)]
  words <- words[nchar(words) > 1]
  words <- words[!grepl("^[0-9]+$", words)]
  words <- words[words != ""]
  
  word_freq <- sort(table(words), decreasing = TRUE)
  
  set.seed(123)
  wordcloud(names(word_freq), freq = word_freq, max.words = 100, scale = c(3, 0.5),
            colors = brewer.pal(8, "Dark2"))
}


visualize_barchart <- function(data, stop_words) {
  df <- data.frame(
    Label = rep(data$Label, sapply(strsplit(data$Body, "\\W"), length)),
    word  = tolower(unlist(strsplit(data$Body, "\\W")))
  )

  df <- df %>% 
    filter(!word %in% stop_words,
           nchar(word) > 1,
           !grepl("^[0-9]+$", word),
           word != "")
  
  df_count <- df %>%
    group_by(Label, word) %>%
    summarise(n = n(), .groups = "drop") %>%
    group_by(Label) %>%
    slice_max(n, n = 10)
  
  ggplot(df_count, aes(x = reorder(word, n), y = n, fill = Label)) +
    geom_col(show.legend = FALSE) +
    coord_flip() +
    facet_wrap(~Label, scales = "free") +
    labs(title = "The most common words in credible/fake news",
         x = "Word", y = "Frequency")
}

visualize_wordcloud(train, "credible", stop_words)
visualize_wordcloud(train, "fake", stop_words)

visualize_barchart(train, stop_words)

```

## Classifier implementation

```{r}
naiveBayes <- setRefClass("naiveBayes",
                          
       # here it would be wise to have some vars to store intermediate result
       # frequency dict etc. Though pay attention to bag of words! 
       fields = list(
         class_probs = "numeric",
         word_probs = "list",
         vocabulary = "character",
         classes = "numeric"
       ),
       methods = list(
                    # prepare your training data as X - bag of words for each of your
                    # messages and corresponding label for the message encoded as 0 or 1 
                    # (binary classification task)
                    fit = function(X, y)
                    {
                        # Ensure X and y have same length
                        if (nrow(X) != length(y)) {
                            stop("X and y must have the same number of samples")
                        }
                        
                        
                        # Store classes and vocabulary
                        classes <<- unique(y)
                        vocabulary <<- colnames(X)
                        
                        # Calculate class probabilities P(class)
                        class_probs <<- sapply(classes, function(c) sum(y == c) / length(y))
                        names(class_probs) <<- classes
                        
                        
                        # Calculate word probabilities P(word|class) for each class
                        word_probs <<- list()
                        
                        for (class in classes) {
                        
                            # Get indices of documents belonging to this class
                            class_indices <- which(y == class)
                            
                            # Get documents belonging to this class
                            if (length(class_indices) == 1) {
                                class_docs <- matrix(X[class_indices, ], nrow = 1)
                                colnames(class_docs) <- colnames(X)
                            } else {
                                class_docs <- X[class_indices, ]
                            }
                            
                            # Calculate total word count for this class
                            total_words <- sum(class_docs)
                            
                            # Calculate word frequencies for this class with Laplace smoothing
                            word_counts <- colSums(class_docs)
                            vocab_size <- length(vocabulary)
                            
                            # Apply Laplace smoothing: (count + 1) / (total + vocab_size)
                            word_prob_class <- (word_counts + 1) / (total_words + vocab_size)
                            
                            word_probs[[as.character(class)]] <<- word_prob_class
                        }
                        
                    },
                    
                    # return prediction for a single message 
                    predict = function(message)
                    {
                         # Handle input - assume it's already a bag of words vector (row from matrix)
                         if (is.vector(message)) {
                           bow <- message
                         } else {
                           bow <- as.vector(message)
                         }
                         
                         # Calculate log probabilities for each class
                         log_probs <- numeric(length(classes))
                         names(log_probs) <- as.character(classes)
                         
                         for (class in classes) {
                           class_key <- as.character(class)
                           
                           # Start with log of class probability
                           log_prob <- log(class_probs[class_key])
                           
                           # Add log probabilities for words that appear (vectorized operation)
                           nonzero_indices <- which(bow > 0)
                           if (length(nonzero_indices) > 0) {
                             log_prob <- log_prob + sum(bow[nonzero_indices] * log(word_probs[[class_key]][nonzero_indices]))
                           }
                           
                           log_probs[class_key] <- log_prob
                         }
                         
                         # Return class with highest probability
                         return(as.numeric(names(which.max(log_probs))))
                    },
                    
                    # score you test set so to get the understanding how well you model
                    # works.
                    # look at f1 score or precision and recall
                    # visualize them 
                    # try how well your model generalizes to real world data! 
                    score = function(X_test, y_test)
                    {
                      if (nrow(X_test) != length(y_test)) {
                        stop("X_test and y_test must have the same number of samples")
                      }
                      
                      y_test <- as.numeric(as.character(y_test))
                      
                      predictions <- numeric(nrow(X_test))
                      for (i in 1:nrow(X_test)) {
                        predictions[i] <- predict(matrix(X_test[i, , drop = FALSE], nrow = 1))
                      }
                      
                      # metrics count
                      TP <- sum(predictions == 1 & y_test == 1)
                      TN <- sum(predictions == 0 & y_test == 0)
                      FP <- sum(predictions == 1 & y_test == 0)
                      FN <- sum(predictions == 0 & y_test == 1)
                      
                      accuracy <- (TP + TN) / (TP + TN + FP + FN)
                      precision <- ifelse((TP + FP) > 0, TP / (TP + FP), 0) # Precision = TP / (TP+FP)
                      recall <- ifelse((TP + FN) > 0, TP / (TP + FN), 0) # Recall = TP / (TP+FN)
                      f1 <- ifelse((precision + recall) > 0, 2 * precision * recall / (precision + recall), 0) # F1 = 2PR / (P+R)

                      cm <- matrix(c(TN, FP, FN, TP), nrow = 2,
                                   dimnames = list(Actual = c("0","1"), Predicted = c("0","1")))
                      
                      cat("\n=== MODEL PERFORMANCE ===\n")
                      cat("Accuracy :", round(accuracy, 3), "\n")
                      cat("Precision:", round(precision, 3), "\n")
                      cat("Recall   :", round(recall, 3), "\n")
                      cat("F1 Score :", round(f1, 3), "\n")
                      
                      return(list(
                        accuracy = accuracy,
                        precision = precision,
                        recall = recall,
                        f1 = f1,
                        confusion_matrix = cm
                      ))
                    }
))
```
```{r}
model = naiveBayes()
model$fit(X_train, y_train)

# Example predictions on individual test samples
cat("\n=== EXAMPLE PREDICTIONS ===\n")

# Example 1
cat("Test sample 1:\n")
cat("Input text:", test$Body[1], "\n")
prediction1 <- model$predict(X_test[1, ])
cat("Predicted class:", prediction1, "\n")
cat("Actual class:", y_test[1], "\n")
cat("Result:", ifelse(prediction1 == y_test[1], "CORRECT", "WRONG"), "\n\n")

# Example 2
cat("Test sample 2:\n")
cat("Input text:", test$Body[2], "\n")
prediction2 <- model$predict(X_test[2, ])
cat("Predicted class:", prediction2, "\n")
cat("Actual class:", y_test[2], "\n")
cat("Result:", ifelse(prediction2 == y_test[2], "CORRECT", "WRONG"), "\n\n")

# Example 3
cat("Test sample 3:\n")
cat("Input text:", test$Body[3], "\n")
prediction3 <- model$predict(X_test[3, ])
cat("Predicted class:", prediction3, "\n")
cat("Actual class:", y_test[3], "\n")
cat("Result:", ifelse(prediction3 == y_test[3], "CORRECT", "WRONG"), "\n")


```

## Measure effectiveness of your classifier

-   Note that accuracy is not always a good metric for your classifier.
    Look at precision and recall curves, F1 score metric.

    When evaluating the model, it's important to understand the
    different types of classification results:

    -   A ***true positive*** result is one where the model correctly
        predicts the positive class.
    -   A ***true negative*** result is one where the model correctly
        predicts the negative class.
    -   A ***false positive*** result is one where the model incorrectly
        predicts the positive class when it is actually negative.
    -   A ***false negative*** result is one where the model incorrectly
        predicts the negative class when it is actually positive.

    Precision measures the proportion of true positive predictions among
    all positive predictions made by the model.

    $$
    Precision = \frac{TP}{TP+FP}
    $$

    Recall, on the other hand, measures the proportion of true positives
    identified out of all actual positive cases.

    $$
    Recall = \frac{TP}{TP+FN}
    $$

    F1 score is the harmonic mean of both precision and recall.

    $$
    F1 = \frac{2\times Precision \times Recall}{Precision + Recall}
    $$

    **See [this
    link](https://cohere.com/blog/classification-eval-metrics) to find
    more information about metrics.**

-   Visualize them.

-   Show failure cases.

```{r}
results <- model$score(X_test, y_test)

# --- Confusion Matrix Heatmap ---
cm <- results$confusion_matrix
class(cm)

# data frame
cm_df <- as.data.frame(as.table(cm))
colnames(cm_df) <- c("Actual", "Predicted", "Freq")

ggplot(cm_df, aes(x = Predicted, y = Actual, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), color = "white", size = 6) +
  scale_fill_gradient(low = "steelblue", high = "darkblue") +
  labs(title = "Confusion Matrix Heatmap") +
  theme_minimal()

metrics_df <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall", "F1 Score"),
  Value = c(results$accuracy, results$precision, results$recall, results$f1)
)

ggplot(metrics_df, aes(x = Metric, y = Value, fill = Metric)) +
  geom_col(width = 0.6, show.legend = FALSE) +
  geom_text(aes(label = round(Value, 3)), vjust = -0.5) +
  ylim(0, 1) +
  labs(title = "Model Performance Metrics", y = "Score", x = "") +
  theme_minimal()


```

## Conclusions

Summarize your work by explaining in a few sentences the points listed
below.

-   **Describe the method implemented in general. Show what are
    mathematical foundations you are basing your solution on.**

    The implemented Naive Bayes classifier is a probabilistic machine learning algorithm based on Bayes' theorem with the "naive" assumption of conditional independence between features. The mathematical foundation relies on Bayes' formula:

    $$\mathsf{P}(\mathrm{class} \mid \mathrm{observation}) = \frac{\mathsf{P}(\mathrm{observation} \mid \mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}$$

    Under the independence assumption, this becomes:

    $$\mathsf{P}(\mathrm{class} \mid \mathrm{observation}) = \mathsf{P}(\mathrm{class})\times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i\mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}$$

    Our implementation calculates class probabilities P(class) and conditional word probabilities P(word|class) from training data, applies Laplace smoothing to handle unseen words, and uses logarithmic probabilities to prevent numerical underflow during computation.

-   **List pros and cons of the method. This should include the
    limitations of your method, all the assumption you make about the
    nature of your data etc.**

    **Pros:**
    - Simple and computationally efficient algorithm
    - Requires relatively small training datasets to estimate parameters
    - Performs well with irrelevant features due to independence assumption
    - Handles multi-class classification naturally
    - Provides probabilistic predictions, not just classifications
    - Works well with text classification tasks like fake news detection

    **Cons and Limitations:**
    - **Strong independence assumption**: Words in text are not actually independent (e.g., "New York" has dependent words)
    - **Limited feature relationships**: Cannot capture interactions between words that might be important for context
    - **Assumes bag-of-words representation**: Loses word order and sentence structure information
    - **Sensitive to skewed data**: Performance can degrade with highly imbalanced datasets
    - **Zero probability problem**: Without smoothing, unseen words would cause zero probabilities
    - **Assumes features follow the same distribution**: May not hold for different types of text documents

-   **Explain why accuracy is not a good choice for the base metrics for
    classification tasks. Why F1 score is always preferable?

    Accuracy is problematic for classification tasks, especially with imbalanced datasets. In such cases,
    a classifier can achieve high accuracy simply by predicting the
    majority class while failing to detect the minority one. The F1 score, as the harmonic mean
    of precision(how many predicted fakes are actually fake) and recall(how many actual fakes are detected), provides a more balanced evaluation by
    penalizing models that sacrifice one metric for the other. This makes F1 particularly valuable for imbalanced datasets
